\section{Fitting Methods}\label{sec:fitting-methods}

To fit the aforementioned models to the data, we employ an algorithm which maximizes the likelihood function,
\begin{equation}
  \mathcal{L}(\vec{\beta}) = e^{-\mathcal{N}} \frac{\mathcal{N}^N}{N!} \prod_{i=1}^{N} \mathcal{P}_i(\vec{\beta})
  \label{eq:likelihood}
\end{equation}
where $N$ is the number of events in the data, $\mathcal{N}$ is the number of events predicted by the model, and $\mathcal{P}_i(\vec{\beta})$ is the normalized probability distribution function evaluated on the $i$th event at position $\vec{\beta}$ in parameter space. The term in front of the product is a Poisson distribution which describes the ``extended'' maximum likelihood method. We relate these probability distributions to our modeled intensity function in \Cref{eq:generalized-polarized-intensity} via the normalization,
\begin{equation}
  \mathcal{I}(\vec{\beta}; m_i, s_i, \Omega_i, P_{\gamma, i}, \Phi_i) \equiv \mathcal{I}_i(\vec{\beta}) = \mathcal{N}\mathcal{P}_i(\vec{\beta})
\end{equation}                                       h

By absorbing the factor of $\mathcal{N}^N$ into the product, we find

\begin{equation}
  \mathcal{L}(\vec{\beta}) = \frac{e^{-\mathcal{N}}}{N!} \prod_{i=1}^{N} \mathcal{I}_i(\vec{\beta})
\end{equation}

We want to maximize this function, but the product makes this computationally difficult and unstable, since it can grow very large (if $\mathcal{I} > 1$) or very small (if $\mathcal{I} < 1$) to a point where its value exceeds floating-point precision. The standard solution is to instead minimize the negative logarithm of the likelihood instead,

\begin{equation}
  -2 \ln \mathcal{L}(\vec{\beta}) = -2 \left(\sum_{i=1}^{N} \left[\ln \mathcal{I}_i(\vec{\beta})\right] - \mathcal{N} - \ln N! \right)
\end{equation}
where the factor of two scales the log-likelihood to correspond to a $\chi^2$ distribution as $N \to \infty$ by Wilks' theorem, allowing us to obtain an accurate covariance matrix from the fit for uncertainty estimation.

In an ideal world, we could stop here, ignoring the last two terms as they are constant in $\beta$ and therefore do not contribute to the gradient of the negative log-likelihood. However, we must also consider the efficiency of the detector itself, which we will define as $\eta(m_i, s_i, \Omega_i, P_{\gamma,i}, \Phi_i) \equiv \eta_i$. In principle, we do not know the analytical form of this function, but we will later see that we can approximate it using Monte Carlo simulated data.

With this efficiency function in mind, we find that $\mathcal{N}$, the normalization factor which describes the number of events predicted by the model, becomes

\begin{equation}
  \mathcal{N} = \int \dd{\vec{x}} \mathcal{I}(\vec{\beta}; \vec{x})\eta(\vec{x})
\end{equation}
where $\vec{x} = (m, s, \Omega, P_{\gamma}, \Phi)$. This means we can write the normalized probability distribution functions as

\begin{equation}
  \mathcal{P}_i(\vec{\beta}) = \frac{1}{\mathcal{N}} \mathcal{I}_i(\vec{\beta})\eta_i
\end{equation}

This changes the negative log-likelihood to

\begin{align}
  - 2 \ln \mathcal{L}(\vec{\beta}) &= -2 \left( \sum_{i=1}^N \left[\ln \mathcal{I}_i(\vec{\beta})\right] - \int \dd{\vec{x}} \mathcal{I}(\vec{\beta; \vec{x}})\eta(\vec{x}) - \ln N! + \sum_{i=1}^N \left[\ln\eta_i\right]\right) \\
                                   &= -2 \left( \sum_{i=1}^N \left[\ln \mathcal{I}_i(\vec{\beta})\right] - \int \dd{\vec{x}} \mathcal{I}(\vec{\beta; \vec{x}})\eta(\vec{x})\right) + C
\end{align}

Next, we need a way of dealing with the integral term, particularly with the $\eta(\vec{x})$ function for which we do not have access to the analytical form. To approximate this integral, we use both the fundamental theorem of calculus and the mean value theorem for derivatives, which together relate the integral of a function to its average value on the domain of integration,

\begin{equation}
  \int_{\mathbb{D}} f(\vec{x})\dd{\vec{x}} = \mathbb{A} \ev{f(\vec{x})}
  \label{eq:mean-value-theorem}
\end{equation}
where $\mathbb{A}$ is the area of the integration domain $\mathbb{D}$ and $\ev{f(\vec{x})}$ denotes the average value of $f(\vec{x})$ on that domain. If we generate phase-space Monte Carlo for the $K_S^0K_S^0$ channel, we can model $\eta(\vec{x})$ as an indicator function corresponding to whether or not an event is detected by a simulation of the detector, passes through the reconstruction process described in \Cref{sub:particle-identification-and-the-gluex-kinematic-fit}, and passes through our finer data selection from \Cref{sub:fiducial-cuts}. We can then write the average as a sum over the ``accepted'' Monte Carlo events,

\begin{equation}
  \int \dd{\vec{x}} \mathcal{I}(\vec{\beta}; \vec{x})\eta(\vec{x}) = \frac{1}{\mathbb{A} N_g} \sum_{j=1}^{N_a} \left[\mathcal{I}_j(\vec{\beta})\right]
\end{equation}

where $\mathcal{I}_j(\vec{\beta})$ is evaluated on the $j$th accepted Monte Carlo event\footnote{This is not to be confused with $\mathcal{I}_i(\vec{\beta})$, which we will use to refer to the function evaluated over data. These terms will always stay in separate sums for clarity.}, and $N_a$ and $N_g$ are the number of accepted and generated events, respectively. We still need to deal with the factor of $\mathbb{A}$. We could absorb this into our intensity function by scaling the intensity by $\mathbb{A}^{N_a}$, which would eliminate it from this term. In the data term, we would then have

\begin{equation}
  \sum_{i=1}^N \left[\mathbb{A}^{N_a}\mathcal{I}_i(\vec{\beta})\right] = \sum_{i=1}^N \left[\mathcal{I}_i(\vec{\beta})\right] + NN_a\ln\mathbb{A}
\end{equation}
where the new additive term is constant in $\vec{\beta}$ and can again be ignored in the minimization. At this stage, we need to minimize

\begin{equation}
  -2\ln\mathcal{L}(\vec{\beta}) = -2\left(\sum_{i=1}^N\left[\ln\mathcal{I}_i(\vec{\beta})\right] - \frac{1}{N_g} \sum_{i=j}^{N_a}\left[\mathcal{I}_j(\vec{\beta})\right]\right)
\end{equation}

We can incorporate event weights (from accidental subtraction or sPlot, for example) into these sums as

\begin{equation}
  -2\ln\mathcal{L}(\vec{\beta}) = -2\left(\sum_{i=1}^N\left[w_i\ln\mathcal{I}_i(\vec{\beta})\right] - \frac{1}{N_g} \sum_{i=j}^{N_a}\left[w_j\mathcal{I}_j(\vec{\beta})\right]\right)
\end{equation}

An additional scaling of the intensity with $\mathcal{I}(\vec{\beta}) \to \varepsilon^{-N_{\alpha}}\mathcal{I}(\vec{\beta})$ with $N_g = \varepsilon N_a$ can be made to eliminate $N_g$ from this expression, which adds a constant factor of $-NN_a\ln\varepsilon$ to the negative log-likelihood. This choice is optional, but it can be helpful from a computational/organizational standpoint when it comes to projecting the fitted model back onto each Monte Carlo dataset and is chosen in this analysis to give us

\begin{equation}
  -2\ln\mathcal{L}(\vec{\beta}) = -2\left(\sum_{i=1}^N\left[w_i\ln\mathcal{I}_i(\vec{\beta})\right] - \frac{1}{N_a} \sum_{i=j}^{N_a}\left[w_j\mathcal{I}_j(\vec{\beta})\right]\right)
\end{equation}

We can now choose a minimization algorithm to fit our data to various models which will be described in detail in \Cref{sec:mass-independent-fits,sec:mass-dependent-fits}. For this analysis, fits were performed with \texttt{laddu}, an amplitude analysis engine I wrote, which uses the L-BFGS-B minimization algorithm, a memory-efficient variant of the Broyden-Fletcher-Goldfarb-Shanno algorithm which allows for box constraints on the parameter space~\cite{byrd_limited_1995}.

To obtain plots of the fitted model, we can weight each event in either the accepted or generated Monte Carlo (for results without and with efficiency correction, respectively) as follows,

\begin{equation}
  \hat{w}_j = \frac{w_j \mathcal{I}_j(\vec{\beta}^*)}{N_\text{MC}}
\end{equation}

where $\vec{\beta}^*$ is the value of the fit parameters which maximizes the likelihood and $N_\text{MC}$ is $N_a$ or $N_g$ depending on which set of Monte Carlo we are using. We can additionally isolate individual waves by manually setting the coefficients of other waves to zero before evaluating the intensity.

\subsection{Uncertainty Estimation}

Quasi-Newton minimization algorithms like L-BFGS-B\footnote{This also applies to other algorithms commonly used in this field, like \texttt{MINUIT}} perform gradient descent by making progressively better estimations of the Hessian matrix $\mathbf{H}$ at each step in the algorithm. One can then obtain an estimate of the uncertainty of the $i$th parameter from $\sigma_i = \sqrt{(\mathbf{H}^{-1})_{ii}}$, for which we can either use the Hessian approximate at the end of the minimization or calculate the true Hessian to some arbitrary level of computational precision using finite differences. This gives us the uncertainty on each fit parameter, but not the uncertainty on the number of modeled counts in a particular bin. Propagating the uncertainty is straightforward mathematically, but complicated computationally. Additionally, the Hessian method does not give us any further detail about the shape of the likelihood surface at the minimum other than the standard deviation of a particular parameter. To examine higher order moments, we require methods which either directly sample the parameter space, e.g. Markov chain Monte Carlo (MCMC), or which resample the data, e.g. the jackknife or bootstrap methods. For this thesis, we will use the bootstrap, as MCMC methods are much more time-consuming and require the tuning of hyperparameters and walker steps, and the jackknife method, which involves resampling the data by omitting one or more events at a time, is only preferred when the bootstrap method described as follows is computationally infeasible.

The bootstrap method derives from the following reasoning. If we had some existing estimate of uncertainty, we could check the estimate by simply collecting more data, performing the analysis on this new data, and observing if our results fall within the estimated uncertainty. However, even if we were to obtain more data, we lack the original estimate of uncertainty. We can obtain such an estimate by ``pulling ourselves up by our bootstraps'' and resampling the data with replacement to obtain a slightly different ``bootstrapped'' dataset, which has the same number of events as the original, but contains some duplicate events and misses some of the original events. Assuming the original data is a representative sample of our population (in this case, assuming a new set of data would only effect the precision of our results), we can obtain several of these bootstrapped datasets and perform our fit. We must start each fit at the true minimum found from the original dataset, since we want to understand the uncertainty as it relates to our model, not to the path the minimizer takes, which may run into different local/false minima in each bootstrapped dataset. The distribution of a parameter over many bootstrapped analyses can then be associated with the uncertainty of that parameter.

We begin by defining the statistic $\hat{\beta}_i(X) = \beta_i^*$ where $X$ represents the dataset and $\hat{\beta}_i$ is the maximum likelihood estimator of $\beta_i$ which maps the events in the dataset to a particular value $\beta_i^*$ via the fitting process we described in \Cref{sec:fitting-methods}. We then resample the data with replacement $B$ times to obtain bootstrapped datasets $\{X_1,\ldots,X_b\}$. The standard error on the parameter $\beta_i$ is then given by~\cite{efron_nonparametric_1981},

\begin{equation}
  \hat{\sigma}_{\beta_i}^{(B)} = \sqrt{\sum_{b=1}^{B}\frac{\left(\hat{\beta}_i(X_b) - \frac{1}{B}\sum_{b'=1}^{B} \hat{\beta}_i(X_{b'}) \right)^2}{B-1}}
  \label{eq:bootstrap-standard-error}
\end{equation}

We can also extract confidence intervals for each parameter using bootstrapping. These might be more useful than our standard error estimates, since the shape of the likelihood surface is not necessarily symmetric. First, we define the cumulative distribution function of the $\hat{\beta}_i$ statistic as~\cite{efron_nonparametric_1981},

\begin{equation}
  \hat{\text{CDF}}_i(t) = \Pr(\hat{\beta}_i < t) = \frac{\#\left\{\hat{\beta}_i(X_b) < t\right\}}{B}
\end{equation}
where $\#\{\cdot\}$ represents the number of values satisfying the enclosed condition\footnote{The last equality here only holds in the limit as $B\to\infty$, but we will ignore this for now.}. Then, for some value $\alpha \in [0, 1]$, we define

\begin{equation}
  \hat{\theta}_i(\alpha) \equiv \hat{\text{CDF}}_i^{-1}(\alpha)
\end{equation}

We then define the $(1 - 2\alpha)\cdot 100\%$ central confidence interval for $\beta_i$ as $\beta_i \in [\hat{\theta}(\alpha),\hat{\theta}(1 - \alpha)]$. Furthermore, we can can define a bias-corrected confidence interval~\cite{efron_nonparametric_1981} which corrects for the fact that the mean of the bootstrap distribution is not necessarily the value obtained from the fit to the original data. First, we define a bias-correction factor,

\begin{equation}
  z_0 = \Phi^{-1}\left(\hat{\text{CDF}}_i(\hat{\beta}_i(X))\right)
\end{equation}
where $\Phi(z)$ is the cumulative distribution function of the standard normal distribution and $\hat{\beta}_i(X)$ is the value of $\beta_i$ we obtain from the fit to the original dataset $X$. Then, the bias-corrected $(1-2\alpha)\cdot 100\%$ central confidence interval is $\beta_i \in \left[\hat{\theta}(\alpha_\text{lo}),\hat{\theta}(\alpha_\text{hi})\right]$ where
\begin{align}
  \alpha_\text{lo} &= \Phi\left(2z_0 + z(\alpha)\right) \\
  \alpha_\text{hi} &= \Phi\left(2z_0 + z(1-\alpha)\right) \\
  z(\alpha) &\equiv \Phi^{-1}(\alpha)
\end{align}

The interpretation of a confidence interval obtained this way is frequentist: If we were to repeat the GlueX experiment many times and evaluated $\beta_i$ for each experiment, we would expect $P\%$ of the values to fall within an $P\%$ confidence interval. The Bayesian approach would be to assign a ``credible interval'', where the probability that an estimate falls in the credible interval is $P\%$.

\section{Mass-Independent Fits}\label{sec:mass-independent-fits}
\section{Mass-Dependent Fits}\label{sec:mass-dependent-fits}
\subsection{Guided Fits}\label{sub:guided-fits}
\section{Systematics}
